# cd /d D:\gitcode\cosyvoice-daxiang027
# conda activate cosyvoice-daxiang027
# python api.py
# http://100.64.0.16:51870/v1/voices


# æ”¯æŒéžæµå¼æŽ¨ç†(æ ‡å‡†OPENAI)
import io
import time
import torch
from fastapi import FastAPI, HTTPException, Query
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import Literal
import soundfile as sf
import sys
sys.path.append('third_party/Matcha-TTS')
from cosyvoice.cli.cosyvoice import CosyVoice, CosyVoice2
from cosyvoice.utils.file_utils import load_wav
import os
from soundfile import info as sfinfo
from fastapi import FastAPI, BackgroundTasks, HTTPException
from fastapi.responses import FileResponse
from pathlib import Path
import asyncio
import uuid
from typing import List, Dict
import os
import math
import aiofiles
import numpy as np
import random
import functools

# ==================== JITå…¼å®¹æ€§è¡¥ä¸ ====================
def patch_encoder_forward(flow_model):
    """
    åŠ¨æ€ä¿®æ”¹ flow_model.encoder.forward å®žä¾‹æ–¹æ³•
    ä½¿å…¶å¿½ç•¥ context å‚æ•°,å…¼å®¹ JIT ç¼–è¯‘åŽçš„æ¨¡åž‹
    """
    if not hasattr(flow_model, 'encoder'):
        print("âš ï¸  æ¨¡åž‹æ²¡æœ‰ encoder å±žæ€§,è·³è¿‡è¡¥ä¸")
        return

    original_forward = flow_model.encoder.forward

    @functools.wraps(original_forward)
    def patched_forward(*args, **kwargs):
        # ç§»é™¤ JIT ä¸æŽ¥å—çš„ context å‚æ•°
        kwargs.pop('context', None)
        return original_forward(*args, **kwargs)

    flow_model.encoder.forward = patched_forward
    print("ðŸ”§ æˆåŠŸåº”ç”¨è¿è¡Œæ—¶è¡¥ä¸: encoder.forward å°†å¿½ç•¥ context å‚æ•°")

app = FastAPI(title="CosyVoice[XDF] TTS API")

# åˆå§‹åŒ–æ¨¡åž‹ (æŒ‰å®žé™…æ¨¡åž‹åŠ è½½æ–¹å¼ä¿®æ”¹)
cosy_voice = None
device = "cuda" if torch.cuda.is_available() else "cpu"

voice_path='./voices'

class TTSRequest(BaseModel):
    model: str = "tts-1"  # ä¿æŒOpenAIå…¼å®¹çš„modelåç§°
    voice: str = "alloy"  # ä¸ºå…¼å®¹ä¿ç•™å‚æ•°,å®žé™…ä½¿ç”¨CosyVoiceçš„é»˜è®¤å£°éŸ³
    input: str
    response_format: Literal["mp3", "flac", "wav"] = "mp3"
    speed: float = 1.0
    # stream å‚æ•°å·²ç§»é™¤,æ”¹ä¸ºæŸ¥è¯¢å‚æ•°

model_dir = 'pretrained_models/CosyVoice2-0.5B'
   
def get_voices():
    path='./voices'
    wav_files = [os.path.splitext(f)[0] for f in os.listdir(path) if f.endswith('.wav')]
    return wav_files

voice_names = get_voices()

# èŽ·å–çŽ¯å¢ƒå˜é‡é…ç½® (é»˜è®¤å€¼ä¿æŒåŽŸæœ‰é€»è¾‘)
load_jit = os.getenv('LOAD_JIT', 'True').lower() == 'true'
load_trt = os.getenv('LOAD_TRT', 'True').lower() == 'true'
fp16 = os.getenv('FP16', 'False').lower() == 'true'

print(f"Loading model {model_dir} ...")
print(f"ðŸš€ å°è¯•å¯ç”¨TensorRTåŠ é€Ÿ (é¦–æ¬¡å¯åŠ¨ä¼šè½¬æ¢æ¨¡åž‹,éœ€5-10åˆ†é’Ÿ)")
print(f"âœ… å¼€å…³é…ç½®: fp16={fp16}, JIT={load_jit}, TRT={load_trt}")
cosyvoice = CosyVoice2(
    model_dir=model_dir, 
    load_jit=load_jit, 
    load_trt=load_trt,
    fp16=fp16
)
print(f"âœ… æ¨¡åž‹å·²åŠ è½½, fp16={fp16}, JIT={load_jit}, TRT={load_trt}")

# åº”ç”¨è¿è¡Œæ—¶è¡¥ä¸ä»¥å…¼å®¹JITç¼–è¯‘
if hasattr(cosyvoice, 'model') and hasattr(cosyvoice.model, 'flow'):
    patch_encoder_forward(cosyvoice.model.flow)
else:
    print("âš ï¸  æ— æ³•æ‰¾åˆ° cosyvoice.model.flow,è·³è¿‡è¡¥ä¸")

# å›ºå®šéšæœºç§å­,ç¡®ä¿éŸ³è‰²ä¸€è‡´
SEED = 0
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# åŠ è½½éŸ³è‰²
for voice_name in voice_names:
    print(f'Loading voice: {voice_name}')
    wav_file=f'./voices/{voice_name}.wav'
    prompt_file=f'./voices/{voice_name}.txt'
    prompt_speech_16k = load_wav(wav_file, 16000)
    prompt_text = open(prompt_file, encoding='utf-8').read()
    cosyvoice.add_zero_shot_spk(
        prompt_text=prompt_text,
        prompt_speech_16k=prompt_speech_16k,
        zero_shot_spk_id=voice_name
    )

print(f'@XDF@æ¨¡åž‹: {model_dir} å·²åŠ è½½')

    
@app.get("/v1/voices")
async def get_voices()->list:
    return voice_names


def create_wav_header(sample_rate: int, channels: int, bits_per_sample: int, data_size: int = 0):
    """åˆ›å»ºWAVæ–‡ä»¶å¤´(44å­—èŠ‚)"""
    import struct
    
    if data_size == 0:
        data_size = 0xFFFFFFFF - 36
    
    header = bytearray()
    header.extend(b'RIFF')
    header.extend(struct.pack('<I', data_size + 36))
    header.extend(b'WAVE')
    header.extend(b'fmt ')
    header.extend(struct.pack('<I', 16))
    header.extend(struct.pack('<H', 3))  # IEEE float
    header.extend(struct.pack('<H', channels))
    header.extend(struct.pack('<I', sample_rate))
    bytes_per_second = sample_rate * channels * (bits_per_sample // 8)
    header.extend(struct.pack('<I', bytes_per_second))
    block_align = channels * (bits_per_sample // 8)
    header.extend(struct.pack('<H', block_align))
    header.extend(struct.pack('<H', bits_per_sample))
    header.extend(b'data')
    header.extend(struct.pack('<I', data_size))
    return bytes(header)


def generate_audio_stream(request: TTSRequest, spk_id: str, response_format: str):
    """éŸ³é¢‘æµç”Ÿæˆå™¨ - çœŸæ­£çš„æµå¼è¾“å‡º (0.1ç§’å»¶è¿Ÿ)"""
    try:
        # ä»Ž0.1ç§’å¢žå¤§åˆ°0.5ç§’,æé«˜GPUæ‰¹å¤„ç†æ•ˆçŽ‡,é™ä½ŽRTF
        sample_threshold = cosyvoice.sample_rate * 0.5  # 0.5ç§’
        
        accumulated_samples = []
        accumulated_length = 0
        is_first_chunk = True
        chunk_count = 0
        
        print(f"[Stream] å¼€å§‹æµå¼ç”Ÿæˆ: {request.input[:30]}...")
        
        for i, segment in enumerate(cosyvoice.inference_zero_shot(
            tts_text=request.input, 
            prompt_text='',
            prompt_speech_16k='',  
            zero_shot_spk_id=spk_id,
            speed=request.speed, 
            stream=True
        )):
            audio_chunk = segment['tts_speech']
            audio_np = audio_chunk.numpy().squeeze().astype(np.float32)
            
            accumulated_samples.append(audio_np)
            accumulated_length += len(audio_np)
            
            if accumulated_length >= sample_threshold:
                merged_audio = np.concatenate(accumulated_samples)
                
                if response_format == 'wav':
                    if is_first_chunk:
                        header = create_wav_header(
                            sample_rate=cosyvoice.sample_rate,
                            channels=1,
                            bits_per_sample=32,
                            data_size=0
                        )
                        audio_bytes = merged_audio.tobytes()
                        yield header + audio_bytes
                        is_first_chunk = False
                        chunk_count += 1
                        print(f"[Stream] é¦–chunk: {len(header)+len(audio_bytes)}B")
                    else:
                        audio_bytes = merged_audio.tobytes()
                        yield audio_bytes
                        chunk_count += 1
                        if chunk_count % 10 == 0:
                            print(f"[Stream] å·²å‘é€ {chunk_count} chunk")
                else:
                    buffer = io.BytesIO()
                    sf.write(buffer, merged_audio, cosyvoice.sample_rate, format=response_format)
                    yield buffer.getvalue()
                    chunk_count += 1
                
                accumulated_samples = []
                accumulated_length = 0
        
        # å‘é€å‰©ä½™éŸ³é¢‘
        if accumulated_samples:
            merged_audio = np.concatenate(accumulated_samples)
            
            if response_format == 'wav':
                if is_first_chunk:
                    header = create_wav_header(
                        sample_rate=cosyvoice.sample_rate,
                        channels=1,
                        bits_per_sample=32,
                        data_size=len(merged_audio) * 4
                    )
                    audio_bytes = merged_audio.tobytes()
                    yield header + audio_bytes
                else:
                    audio_bytes = merged_audio.tobytes()
                    yield audio_bytes
                chunk_count += 1
            else:
                buffer = io.BytesIO()
                sf.write(buffer, merged_audio, cosyvoice.sample_rate, format=response_format)
                yield buffer.getvalue()
        
        print(f"[Stream] å®Œæˆ! å…± {chunk_count} chunk")
            
    except Exception as e:
        print(f"æµå¼é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(500, f"Streaming failed: {str(e)}")


@app.post("/v1/audio/speech")
async def generate_speech(
    request: TTSRequest,
    stream: bool = Query(False, description="æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º")
):
    # å‚æ•°éªŒè¯
    if len(request.input) == 0:
        raise HTTPException(400, "Input text cannot be empty")
    if len(request.input) > 4096:
        raise HTTPException(400, "Input text too long (max 4096 characters)")
    if not 0.50 <= request.speed <= 2.0:
        raise HTTPException(400, "Speed must be between 0.50 and 2.0")
    
    spk_id = request.voice
    if spk_id not in voice_names:
        spk_id = 'default'
    
    # æµå¼å“åº”
    if stream:
        if request.response_format not in ['mp3', 'wav']:
            raise HTTPException(400, "Streaming only supports mp3 and wav formats")
        
        mime_map = {
            "mp3": "audio/mpeg",
            "wav": "audio/wav"
        }
        
        return StreamingResponse(
            generate_audio_stream(request, spk_id, request.response_format),
            media_type=mime_map[request.response_format],
            headers={
                "Content-Disposition": f"attachment; filename={spk_id}.{request.response_format}",
                "Cache-Control": "no-cache",
                "X-Accel-Buffering": "no",
                "Transfer-Encoding": "chunked"
            }
        )
    
    # éžæµå¼å“åº”
    try:
        audio_segments = []
        for i, segment in enumerate(cosyvoice.inference_zero_shot(
            tts_text=request.input, 
            prompt_text='',
            prompt_speech_16k='',  
            zero_shot_spk_id=spk_id,
            speed=request.speed, 
            stream=False)):
            
            audio_segments.append(segment['tts_speech'])

        merged_audio = torch.cat(audio_segments, dim=1)  
        merged_audio = merged_audio.numpy().squeeze()   

        buffer = io.BytesIO()
        sf.write(
            buffer,
            merged_audio,
            cosyvoice.sample_rate, 
            format=request.response_format
        )
        buffer.seek(0)

        mime_map = {
            "mp3": "audio/mpeg",
            "wav": "audio/wav",
            "flac": "audio/flac"
        }
        if request.response_format not in mime_map:
            raise HTTPException(400, f"Unsupported format: {request.response_format}")

        return StreamingResponse(
            content=buffer,
            media_type=mime_map[request.response_format],
            headers={
                "Content-Disposition": f"attachment; filename={spk_id}.{request.response_format}" 
            }
        )
    except Exception as e:
        raise HTTPException(500, f"Audio generation failed: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=51870)
